<!DOCTYPE html>
<html>

  <head>
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116924853-1"></script>
      
      <!--<script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-116924853-1');
          </script>-->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
<link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
<meta name="viewport" content="width=device-width, initial-scale=0.45, maximum-scale=1.0">
<link rel="stylesheet" type="text/css" href="style.css">
<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link rel="shortcut icon" type="image/ico" href="favicon.ico" />
<link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
<link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

<style>
    /* Scrollable container */
    .activities-container {
    max-height: 200px; /* Limit height */
    overflow-y: scroll; /* Always show scrollbar */
    padding: 10px;
    width: 100%;
    margin: auto;

    /* Force scrollbar visibility */
    scrollbar-width: thin;  /* For Firefox */
    scrollbar-color: #888 #f1f1f1;
}

/* For WebKit Browsers (Chrome, Edge, Safari) */
.activities-container::-webkit-scrollbar {
    width: 8px; /* Always visible scrollbar */
}

.activities-container::-webkit-scrollbar-thumb {
    background-color: #888; /* Scrollbar thumb color */
    border-radius: 4px;
}

.activities-container::-webkit-scrollbar-track {
    background-color: #f1f1f1; /* Track color */
}
.custom-list {
    text-align: left;
    list-style: none;
    padding-left: 0;
}

.custom-list li {
    display: flex;
    align-items: center;
}

.bullet {
    font-weight: bold;
    font-size: 1.2em;
    margin-right: 5px;
}
.sub-list {
    list-style-type: disc;
    padding-left: 20px;
}
</style>

<title>Saptarshi Sinha</title>
</head>
<body align="center">
    <div id="mainBar">
        <div id="textBar">
        <header>
        <h1>
            <img src="IMG_9063.JPG" alt="Saptarshi Sinha's Photo" style="width:250px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
        </h1>
        <h1>Saptarshi Sinha</h1>
        <!-- <h2 style="text-align:left">Phd Student of Computer Vision</h2> -->
        </header>
        <p class="text-justify" style="text-align:justify">I am currently a Ph.D. student working with Prof. <a href="https://dimadamen.github.io/"> Dima Damen </a> at the School of Computer Science, University of Bristol. I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
            <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>. 
            I was previously a computer vision researcher at <a href="https://www.hitachi.com/rd/">Hitachi Research and Development</a>, Japan from October 2018 to August 2022.
            My research interest lies in computer vision, focusing primarily on long-term video understanding. I completed my Masters in 2018 from <a href="https://www.iitb.ac.in/">IIT Bombay</a> under the supervision of Prof. <a href="https://www.ee.iitb.ac.in/~sc/">Subhasis Chaudhuri</a>. </p>
        <ul class="list-inline list-social-icons mb-0">
            <!-- <li class="list-inline-item">You can find me here <span style='font-size:30px;'>&#128073;</span></li> -->
            
            <li class="list-inline-item">
                <a href="mailto:saptarshi.sinha@bristol.ac.uk">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-envelope fa-stack-1x fa-inverse" ></i>
                    </span>
                </a>
            </li>


            <li class="list-inline-item">
                <a href="https://github.com/sinhasaptarshi">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li>

            <li class="list-inline-item">
                <a href="https://scholar.google.com/citations?user=LFXdBPoAAAAJ&hl=en">
                    <span class="fa-stack fa-lg">
                        <i class="ai ai-google-scholar-square ai-2x"></i>
                        <!-- <img src="https://scholar.google.com/favicon.ico" alt="Google Scholar Icon"> -->
                    </span>
                </a>
            </li>

            <!-- <li class="list-inline-item">
                <a href="pdfs/cv.pdf">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li> -->

            

            <li class="list-inline-item">
                <a href="https://twitter.com/sinha_saptarshi">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li>

            <li class="list-inline-item">
                <a href="https://www.linkedin.com/in/saptarshi-sinha-ab42a410b/">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-square fa-stack-2x"></i>
                        <i class="fa fa-linkedin-square fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </li>
        </ul>
    
        <!-- <h4> Email: saptarshi (dot) sinha (at) bristol (dot) ac (dot) uk </h4> -->


    <hr>
    <h3>Activities</h3>

    <div class="activities-container">

            <!--activities_section-->
            <ul class="text-justify" class="no-bullets" style="text-align:justify">
                <li><span class="text-justify" class="prim-colour">Febraury 2025</span> - <b>HD-Epic out on ArXiv &#x1F389;.</b> More details <a href="https://hd-epic.github.io">here</a>. </li>
                <li><span class="text-justify" class="prim-colour">December 2024</span> - <b>Attended and presented at ACCV, Vietnam.</b> </li>
                <li><span class="text-justify" class="prim-colour">September 2024</span> - <b>ESCounts accepted at ACCV.</b> Our paper "Every Shot Counts: Using Exemplars for Repetition Counting in Videos" got accepted in ACCV. More information <a href="https://sinhasaptarshi.github.io/escounts/">here</a>. </li>
                <li><span class="text-justify" class="prim-colour">April 2024</span> - <b>New paper out on arXiv.</b> Our paper "Every Shot Counts: Using Exemplars for Repetition Counting in Videos" is up on arXiv. More information <a href="https://sinhasaptarshi.github.io/escounts/">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">August 2023</span> - <b>Paper accepted at BMVC 2023.</b> Our paper "MILA: Memory-Based Instance-Level Adaptation for Cross-Domain Object Detection" got accepted in BMVC. More information <a href="https://arxiv.org/abs/2309.01086">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">June 2023</span> - <b>Attended and presented at CVPR, Vancouver.</b> It was an honour presenting at CVPR and meeting new people. More information and photos <a href="https://twitter.com/dimadamen/status/1671342240604762112">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">February 2023</span> - <b>Paper accepted at CVPR 2023.</b> Our paper "Use Your Head: Improving Long-Tail Video Recognition" got accepted in CVPR. More information <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.html">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">September 2022</span> - <b>Started as Ph.D. at University of Bristol.</b> </li>
                <li> <span class="text-justify" class="prim-colour">August 2022</span> - <b>Paper accepted at WACV 2023.</b> Our paper "Difficulty-Net: Learning to Predict Difficulties for Long-Tailed Recognition" got accepted in Round-1 of WACV. More information <a href="https://arxiv.org/abs/2209.02960">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">August 2022</span> - <b>Published in IJCV.</b> Our paper "Class-difficulty based methods for long-tailed visual recognition" got published in IJCV. More information (<a href="https://link.springer.com/article/10.1007/s11263-022-01643-3">here</a>).</li>
                <li> <span class="text-justify" class="prim-colour">April 2021</span> - <b>Co-organised MMAct Challenge in conjunction with ActivityNet@CVPR2021</b> - Cross-modal video action recognition/localisation challenges on the MMAct dataset. Find more information <a href="https://mmact19.github.io/challenge/">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">November 2020</span> - <b>Presented at ACCV 2020.</b> - Nice experience presenting my first international paper.</li>
                <li> <span class="text-justify" class="prim-colour">September 2020</span> - <b>Paper Accepted at ACCV 2020. My first paper.</b> - Our paper titled "Class-wise difficulty-balanced loss for solving class-imbalance" was accepted at ACCV 2020. See more <a href="https://arxiv.org/abs/2010.01824">here</a>.</li>
                <li> <span class="text-justify" class="prim-colour">October 2019</span> - <b>Attended ICCV, Korea.</b> </li>
                <li> <span class="text-justify" class="prim-colour">August 2019</span> - <b>Paper published at IEICE Conferences.</b> You can find the paper <a href="https://www.ieice.org/publications/search/summary.php?id=FIT0000013722&tbl=conf_fit&lang=en">here</a>. </li>
                <li> <span class="text-justify" class="prim-colour">September 2018</span> - <b>Started as a researcher at Hitachi R&D, Japan.</a></b> Supervised by <a href="https://scholar.google.com/citations?user=GKC6bbYAAAAJ&hl=en"> Hiroki Ohashi</a> and <a href="https://scholar.google.com/citations?user=ZIxQ5zAAAAAJ&hl=en">Katsuyuki Nakamura</a> </li>
                <!-- <li> <span class="prim-colour">August 2018</span> - <b>Graduated from IIT Mumbai</b> </li> -->
                
                </ul>
        </div>

    <!-- <table>
        <tr> -->
            <!-- <td><span style='font-size:80px;'>&#9997;</span></td> -->
        <hr>
        <h3>Publications</h3>
    <!-- </tr>
    </table> -->
    <table class="researchtable" style="text-align:justify">
        <tbody>
        </tr>
        <td class=img>
            <img src="images/hd-epic.png" style="width:300px; height:150px">
        </td>
        <td valign="top">
            <span class="text-justify" class="prim-colour">HD-EPIC: A Highly-Detailed Egocentric Video Dataset</span>
            <br>
            Toby Perrett<sup>*</sup>, Ahmad Darkhalil<sup>*</sup>, <span>Saptarshi Sinha</span><sup>*</sup>, Omar Emara<sup>*</sup>, Sam Pollard<sup>*</sup>, Kranti Parida<sup>*</sup>, Kaiting Liu<sup>*</sup>, Prajwal Gatti<sup>*</sup>, Siddhant Bansal<sup>*</sup>, Kevin Flanagan<sup>*</sup>, Jacob Chalk<sup>*</sup>, Zhifan Zhu<sup>*</sup>, Rhodri Guerrier<sup>*</sup>, Fahd Abdelazim<sup>*</sup>, Bin Zhu, Davide Moltisanti, Michael Wray, Hazel Doughty, Dima Damen
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025</em>
            <br>
            <strong>
                <a href="https://hd-epic.github.io">[Webpage]</a>
                <a href="https://arxiv.org/abs/2502.04144">[arXiv]</a>
                <a href="https://github.com/hd-epic/hd-epic-annotations">[github]</a>
            </strong>
        </td>
    </tr>
        <!-- <td class=img>
            <img src="images/everyshotcounts.png" style="width:300px; height:140px">
        </td> -->
        <td>
        <video width="300" height="300" autoplay muted controls loop>
            <source src="images/escounts.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </td>
        <td valign="top">
            <span class="text-justify" class="prim-colour">Every Shot Counts: Using Exemplars for Repetition Counting in Videos</span>
            <br>
            <span>Saptarshi Sinha</span>, Alexandros Stergiou, Dima Damen
            <br>
            <em>Asian Conference of Computer Vision (ACCV), 2024</em>
            <br>
            <strong>
                <a href="https://sinhasaptarshi.github.io/escounts/">[Webpage]</a>
                <a href="https://arxiv.org/abs/2403.18074">[arXiv]</a>
                <a href="https://github.com/sinhasaptarshi/EveryShotCounts">[code]</a>
            </strong>
        </td>
    </tr>
        
        <td class=img>
            <img src="images/mila.png" style="width:300px; height:150px">
        </td>
        <td valign="top">
            <span class="text-justify" class="prim-colour">MILA: memory-based instance-level adaptation for cross-domain object detection</span>
            <br>
            Onkar Krishna, Hiroki Ohashi, <span>Saptarshi Sinha</span>
            <br>
            <em>British Machine Vision Conference (BMVC), 2023</em>
            <br>
            <strong>
                <!--<a href="https://qinghonglin.github.io/EgoVLP/">[Webpage]</a>-->
                <a href="https://arxiv.org/abs/2309.01086">[arXiv]</a>
                <a href="https://github.com/hitachi-rd-cv/MILA">[code]</a>
            </strong>
        </td>
    </tr>


            <td class=img>
                    <img src="images/long-tail-video.png" style="width:300px; height:200px">
                </td>
            <td valign="top", halign="left">
                <span class="text-justify" class="prim-colour">Use your head: Improving long-tail video recognition</span>
                <br>
                Toby Perrett,<span> Saptarshi Sinha</span>,  Tilo Burghardt, Majid Mirmehdi, Dima Damen
                <br>
                <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
                <br>
                <strong>
                    <!--<a href="https://ego-exo4d-data.org/">[Webpage]</a>-->
                    <a href="https://arxiv.org/abs/2304.01143">[arXiv]</a>
                </strong>
                <strong>
                    <!--<a href="https://ego-exo4d-data.org/">[Webpage]</a>-->
                    <a href="https://github.com/tobyperrett/lmr-release">[code]</a>
                </strong>
                <strong>
                    <a href="https://github.com/tobyperrett/lmr">[page]</a>
                </strong>
            </td>

        </tr>
        <td class=img>
            <img src="images/difficulty_net.png" style="width:300px; height:200px">
        </td>
        <td valign="top">
            <span class="text-justify" class="prim-colour">Difficulty-Net: Learning to Predict Difficulty for Long-Tailed Recognition</span>
            <br>
            <span>Saptarshi Sinha</span>, Hiroki Ohashi
            <br>
            <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023</em>
            <br>
            <strong>
                <!--<a href="https://keflanagan.github.io/CliMer-TSG/">[Webpage]</a>-->
                <a href="https://arxiv.org/abs/2209.02960">[arXiv]</a>
                <a href="https://github.com/sinhasaptarshi/Difficulty_Net">[code]</a>
            </strong>
        </td>

    </tr>
    <td class=img>
        <img src="images/ijcv.png" style="width:300px; height: 200px">
    </td>
    <td valign="top">
        <span class="text-justify" class="prim-colour">Class-difficulty based methods for long-tailed visual recognition</span>
        <br>
        <span>Saptarshi Sinha</span>, Hiroki Ohashi, Katsuyuki Nakamura
        <br>
        <em>International Journal of Computer Vision (IJCV), 2022</em>
        <br>
        <strong>
            <!--<a href="https://soczech.github.io/genhowto/">[Webpage]</a>-->
            <a href="https://arxiv.org/abs/2207.14499">[arXiv]</a>
            <a href="https://github.com/sinhasaptarshi/CDB-methods">[code]</a>
        </strong>
    </td>
            
            <!--end_activities-->
    <!--research_section-->
            <tr>
                <td class=img>
                    <img src="images/accv.png" style="width:300px; height:150px">
                </td>
                <!-- <td>
                    <span style='font-size:180px;'>&#128187;</span>
                </td> -->
                <td valign="top">
                    <span class="text-justify" class="prim-colour">Class-wise difficulty-balanced loss for solving class-imbalance</span>
                    <br>
                    <span>Saptarshi Sinha</span>, Hiroki Ohashi, Katsuyuki Nakamura
                    <br>
                    <em>Asian Conference on Computer Vision (ACCV), 2020</em>
                    <br>
                    <strong>
                        <a href="https://arxiv.org/abs/2010.01824">[arXiv]</a>
                        <a href="https://github.com/sinhasaptarshi/CDB-methods">[code]</a>
                    </strong>
                </td>
            
            <!-- </tr>
                <td class=img>
                    <img src="img/ego-exo4d_intro_fig.png">
                </td>
               
             -->
            </tr>
                <td class=img>
                    <img src="images/trecvid.png" style="width:300px; height:100px">
                </td>
                <td valign="top">
                    <span class="text-justify" class="prim-colour">NII Hitachi UIT at TRECVID 2019</span>
                    <br>
                    Martin Klinkigt, Duy-Dinh Le, Atsushi Hiroike, Hung-Quoc Vo, Mohit Chabra, Vu-Minh-Hieu Dang, Quan Kong, Vinh-Tiep Nguyen, Tomokazu Murakami, Tien-Van Do 0002, Tomoaki Yoshinaga, Duy-Nhat Nguyen,<span> Sinha Saptarshi</span>, Thanh-Duc Ngo, Charles Limasanches, Tushar Agrawal, Jian Vora, Manikandan Ravikiran, Zheng Wang, Shin'ichi Satoh 
                    <br>
                    <em>TRECVID, 2019</em>
                    <br>
                    <strong>
                        <!--<a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">[Webpage]</a>-->
                        <a href="https://openreview.net/forum?id=8TKXl2RClV">[OpenReview]</a>
                        <!--<a href="https://github.com/adrianofragomeni/ConTra">[Code]</a>-->
                    </strong>
                </td>
            
    </tbody>
    </table>
    <!--end_research-->

    <hr>
    <h3>Miscellaneous</h3>
    <ul class="custom-list">
        <li><span class="bullet">*</span> Reviewer: CVPR 2025, ICCV 2023, ICCV 2025, ECCV 2024, ACCV 2024, IJCV 2024, TIP 2024, NeuroComputing 2024, PRLetters 2023</li>
        <li><span class="bullet">*</span> Teaching Assistant: Applied Deep Learning, Machine Learning, Applied Data Science</li>
        <li><span class="bullet">*</span> Appointed representative of approximately 300 post-graduate research students funded by EPSRC </li>
        <!-- <li><span class="bullet">*</span> Enhanced model performance through fine-tuning</li>
        <li><span class="bullet">*</span> Conducted extensive debugging and error analysis</li>
        <li><span class="bullet">*</span> Improved computational efficiency in large-scale simulations</li> -->
    </ul>

    <!-- <table>
        <tr> -->
            <!-- <td><span style='font-size:80px;'>&#9997;</span></td> -->
        <hr>
        </tbody>
    </table>
    </div>
    </div>
</body>
